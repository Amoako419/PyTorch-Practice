{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.788500  [   64/60000]\n",
      "loss: 0.867856  [ 6464/60000]\n",
      "loss: 0.645411  [12864/60000]\n",
      "loss: 0.841368  [19264/60000]\n",
      "loss: 0.746590  [25664/60000]\n",
      "loss: 0.730552  [32064/60000]\n",
      "loss: 0.825501  [38464/60000]\n",
      "loss: 0.794330  [44864/60000]\n",
      "loss: 0.802667  [51264/60000]\n",
      "loss: 0.761857  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.760993 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.751015  [   64/60000]\n",
      "loss: 0.838636  [ 6464/60000]\n",
      "loss: 0.614325  [12864/60000]\n",
      "loss: 0.817744  [19264/60000]\n",
      "loss: 0.726241  [25664/60000]\n",
      "loss: 0.705262  [32064/60000]\n",
      "loss: 0.801400  [38464/60000]\n",
      "loss: 0.778281  [44864/60000]\n",
      "loss: 0.780497  [51264/60000]\n",
      "loss: 0.740929  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.739066 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.718232  [   64/60000]\n",
      "loss: 0.812455  [ 6464/60000]\n",
      "loss: 0.587870  [12864/60000]\n",
      "loss: 0.797845  [19264/60000]\n",
      "loss: 0.708397  [25664/60000]\n",
      "loss: 0.684410  [32064/60000]\n",
      "loss: 0.779227  [38464/60000]\n",
      "loss: 0.764030  [44864/60000]\n",
      "loss: 0.761128  [51264/60000]\n",
      "loss: 0.722231  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.719603 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.689041  [   64/60000]\n",
      "loss: 0.788560  [ 6464/60000]\n",
      "loss: 0.564847  [12864/60000]\n",
      "loss: 0.780491  [19264/60000]\n",
      "loss: 0.692465  [25664/60000]\n",
      "loss: 0.666772  [32064/60000]\n",
      "loss: 0.758693  [38464/60000]\n",
      "loss: 0.750909  [44864/60000]\n",
      "loss: 0.743904  [51264/60000]\n",
      "loss: 0.705223  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.701958 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.662562  [   64/60000]\n",
      "loss: 0.766584  [ 6464/60000]\n",
      "loss: 0.544508  [12864/60000]\n",
      "loss: 0.764960  [19264/60000]\n",
      "loss: 0.678140  [25664/60000]\n",
      "loss: 0.651443  [32064/60000]\n",
      "loss: 0.739283  [38464/60000]\n",
      "loss: 0.738918  [44864/60000]\n",
      "loss: 0.728451  [51264/60000]\n",
      "loss: 0.689613  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.685757 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.638426  [   64/60000]\n",
      "loss: 0.746068  [ 6464/60000]\n",
      "loss: 0.526365  [12864/60000]\n",
      "loss: 0.750780  [19264/60000]\n",
      "loss: 0.665152  [25664/60000]\n",
      "loss: 0.638078  [32064/60000]\n",
      "loss: 0.720830  [38464/60000]\n",
      "loss: 0.727868  [44864/60000]\n",
      "loss: 0.714552  [51264/60000]\n",
      "loss: 0.675191  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.670776 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.616440  [   64/60000]\n",
      "loss: 0.726896  [ 6464/60000]\n",
      "loss: 0.509987  [12864/60000]\n",
      "loss: 0.737696  [19264/60000]\n",
      "loss: 0.653300  [25664/60000]\n",
      "loss: 0.626236  [32064/60000]\n",
      "loss: 0.703351  [38464/60000]\n",
      "loss: 0.717711  [44864/60000]\n",
      "loss: 0.702100  [51264/60000]\n",
      "loss: 0.661639  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.656893 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.596264  [   64/60000]\n",
      "loss: 0.709045  [ 6464/60000]\n",
      "loss: 0.495143  [12864/60000]\n",
      "loss: 0.725638  [19264/60000]\n",
      "loss: 0.642458  [25664/60000]\n",
      "loss: 0.615704  [32064/60000]\n",
      "loss: 0.686836  [38464/60000]\n",
      "loss: 0.708558  [44864/60000]\n",
      "loss: 0.690981  [51264/60000]\n",
      "loss: 0.648898  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.644034 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.577708  [   64/60000]\n",
      "loss: 0.692448  [ 6464/60000]\n",
      "loss: 0.481557  [12864/60000]\n",
      "loss: 0.714419  [19264/60000]\n",
      "loss: 0.632541  [25664/60000]\n",
      "loss: 0.606323  [32064/60000]\n",
      "loss: 0.671337  [38464/60000]\n",
      "loss: 0.700344  [44864/60000]\n",
      "loss: 0.681139  [51264/60000]\n",
      "loss: 0.636936  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.632151 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.560762  [   64/60000]\n",
      "loss: 0.677131  [ 6464/60000]\n",
      "loss: 0.469244  [12864/60000]\n",
      "loss: 0.703883  [19264/60000]\n",
      "loss: 0.623388  [25664/60000]\n",
      "loss: 0.597877  [32064/60000]\n",
      "loss: 0.656721  [38464/60000]\n",
      "loss: 0.693044  [44864/60000]\n",
      "loss: 0.672438  [51264/60000]\n",
      "loss: 0.625516  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.621176 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.545270  [   64/60000]\n",
      "loss: 0.662939  [ 6464/60000]\n",
      "loss: 0.457999  [12864/60000]\n",
      "loss: 0.694002  [19264/60000]\n",
      "loss: 0.614852  [25664/60000]\n",
      "loss: 0.590209  [32064/60000]\n",
      "loss: 0.643059  [38464/60000]\n",
      "loss: 0.686731  [44864/60000]\n",
      "loss: 0.664831  [51264/60000]\n",
      "loss: 0.614791  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.611056 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.530974  [   64/60000]\n",
      "loss: 0.649839  [ 6464/60000]\n",
      "loss: 0.447654  [12864/60000]\n",
      "loss: 0.684828  [19264/60000]\n",
      "loss: 0.607050  [25664/60000]\n",
      "loss: 0.583178  [32064/60000]\n",
      "loss: 0.630347  [38464/60000]\n",
      "loss: 0.681387  [44864/60000]\n",
      "loss: 0.658237  [51264/60000]\n",
      "loss: 0.604588  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.601731 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.517815  [   64/60000]\n",
      "loss: 0.637799  [ 6464/60000]\n",
      "loss: 0.438240  [12864/60000]\n",
      "loss: 0.676239  [19264/60000]\n",
      "loss: 0.599720  [25664/60000]\n",
      "loss: 0.576663  [32064/60000]\n",
      "loss: 0.618511  [38464/60000]\n",
      "loss: 0.676875  [44864/60000]\n",
      "loss: 0.652574  [51264/60000]\n",
      "loss: 0.594873  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.593136 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.505580  [   64/60000]\n",
      "loss: 0.626688  [ 6464/60000]\n",
      "loss: 0.429562  [12864/60000]\n",
      "loss: 0.668086  [19264/60000]\n",
      "loss: 0.592704  [25664/60000]\n",
      "loss: 0.570605  [32064/60000]\n",
      "loss: 0.607552  [38464/60000]\n",
      "loss: 0.673180  [44864/60000]\n",
      "loss: 0.647741  [51264/60000]\n",
      "loss: 0.585562  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.585206 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.494208  [   64/60000]\n",
      "loss: 0.616437  [ 6464/60000]\n",
      "loss: 0.421502  [12864/60000]\n",
      "loss: 0.660347  [19264/60000]\n",
      "loss: 0.585960  [25664/60000]\n",
      "loss: 0.564850  [32064/60000]\n",
      "loss: 0.597419  [38464/60000]\n",
      "loss: 0.670214  [44864/60000]\n",
      "loss: 0.643540  [51264/60000]\n",
      "loss: 0.576560  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.577872 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.483586  [   64/60000]\n",
      "loss: 0.606969  [ 6464/60000]\n",
      "loss: 0.414016  [12864/60000]\n",
      "loss: 0.652981  [19264/60000]\n",
      "loss: 0.579523  [25664/60000]\n",
      "loss: 0.559365  [32064/60000]\n",
      "loss: 0.588056  [38464/60000]\n",
      "loss: 0.667872  [44864/60000]\n",
      "loss: 0.639910  [51264/60000]\n",
      "loss: 0.567942  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.571078 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.473664  [   64/60000]\n",
      "loss: 0.598218  [ 6464/60000]\n",
      "loss: 0.407124  [12864/60000]\n",
      "loss: 0.645968  [19264/60000]\n",
      "loss: 0.573203  [25664/60000]\n",
      "loss: 0.554078  [32064/60000]\n",
      "loss: 0.579420  [38464/60000]\n",
      "loss: 0.666104  [44864/60000]\n",
      "loss: 0.636717  [51264/60000]\n",
      "loss: 0.559641  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.564775 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.464370  [   64/60000]\n",
      "loss: 0.590153  [ 6464/60000]\n",
      "loss: 0.400666  [12864/60000]\n",
      "loss: 0.639261  [19264/60000]\n",
      "loss: 0.566965  [25664/60000]\n",
      "loss: 0.548974  [32064/60000]\n",
      "loss: 0.571424  [38464/60000]\n",
      "loss: 0.664828  [44864/60000]\n",
      "loss: 0.633883  [51264/60000]\n",
      "loss: 0.551598  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.558919 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.455603  [   64/60000]\n",
      "loss: 0.582672  [ 6464/60000]\n",
      "loss: 0.394619  [12864/60000]\n",
      "loss: 0.632772  [19264/60000]\n",
      "loss: 0.560793  [25664/60000]\n",
      "loss: 0.544002  [32064/60000]\n",
      "loss: 0.564019  [38464/60000]\n",
      "loss: 0.664001  [44864/60000]\n",
      "loss: 0.631357  [51264/60000]\n",
      "loss: 0.543848  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.553467 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.447336  [   64/60000]\n",
      "loss: 0.575665  [ 6464/60000]\n",
      "loss: 0.388928  [12864/60000]\n",
      "loss: 0.626555  [19264/60000]\n",
      "loss: 0.554757  [25664/60000]\n",
      "loss: 0.539170  [32064/60000]\n",
      "loss: 0.557130  [38464/60000]\n",
      "loss: 0.663486  [44864/60000]\n",
      "loss: 0.629125  [51264/60000]\n",
      "loss: 0.536375  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.548386 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
